{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Final Report.ipynb","provenance":[{"file_id":"https://github.com/adamkritz/nhlgames/blob/sahara/Final%20Report%20(.ipynb)/Final_Report.ipynb","timestamp":1649720622594}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["yoooo did you know you can save directly from colab to github? kinda sick. the path is: Final Report (.ipynb)/Final_Report.ipynb"],"metadata":{"id":"BYj_5kLveUvC"}},{"cell_type":"markdown","source":["# MISC NOTES:\n","- I'm not stratifying rn because it's equal win/loss, but we can try that if things look bad"],"metadata":{"id":"Ug63Yo6tN1PN"}},{"cell_type":"markdown","source":["# Introduction"],"metadata":{"id":"yHlDlg9tWeXk"}},{"cell_type":"markdown","source":["## Motivation and Background"],"metadata":{"id":"Yt7V84DEaHHL"}},{"cell_type":"markdown","source":["Hockey predictions have become a hot topic in recent years. Hockey fans praise hockey's unpredictability, citing how exciting it is to know that any team has a chance. This is usually contrasted with sports like basketball, where it is rare to for underdogs to win. [Here is a video](https://www.youtube.com/watch?v=c4fFOu8nyeM) of former NBA star and current NBA commentator Charles Barkley talking about why he enjoys hockey playoffs, saying that \"you have no idea who is going to win.\" Vox put out [a video](https://www.youtube.com/watch?v=HNlgISa9Giw) on this phenomenon as well, claiming that the reason that hockey underdogs do better than basketball is due to a higher luck element in hockey. \n","\n","But is hockey really so difficult to predict? Compared to other sports, hockey analytics are minimal. The few hockey betting sites that exist use extremely simple algorithms to predict which team will win, and often get it wrong. There is very little other published work about hockey analysis, mostly results from other projects with varying success. Either teams and other groups are not using complex analytics, or they do not want to publish their work publicly. \n","\n","Aside from leveraging models to aid in sports betting and playoff predictions, having an accurate prediction tool can be useful for teams in post-game analysis. Models can highlight patterns to success or failure that might not be obvious when looking only at raw statistics.\n","\n","Our project will attempt to answer whether hockey is a difficult sport to predict. We will use multiple modeling techniques and select the best model to predict the result of hockey games.\n"],"metadata":{"id":"uxw9vFfRlugF"}},{"cell_type":"markdown","source":["## Data Source and Description"],"metadata":{"id":"wLVN9jN0Z3yf"}},{"cell_type":"markdown","source":["Our data comes from the [NHL Game Dataset](https://www.kaggle.com/datasets/martinellis/nhl-game-data) on Kaggle. The dataset was uploaded by Martin Ellis and most recently updated a year ago. The dataset contains data pulled from NHL.com. \n","\n","The dataset is incredibly robust, and contains multiple .csv files full of data on players, coaches, teams, and games. For our project, we will only be using three of these files, games.csv, game_teams_stats.csv, and team_info.csv. Games.csv contains information about he games that happened, game_teams_stats.csv contains information about the teams that played in those games, and team_info.csv is the connecting set for these two files. \n","\n"],"metadata":{"id":"9GSXurLZjzsu"}},{"cell_type":"markdown","source":["## NHL and Hockey Terms"],"metadata":{"id":"r7oMg0ZKZzrE"}},{"cell_type":"markdown","source":["The NHL contains now 32 teams in the US and Canada, though our project will only include data on 31 of these teams, as the Seattle Kraken only joined the league in 2022. It is also important to note that some of these teams have moved, disbanded, or been created throughout the years. For example, the Atlanta Thrashers moved to Winnipeg and became the Jets in 2011. Teams that moved to a new location will be treated as different teams. \n","\n","NHL teams play 82 games in the regular season, barring any lockouts, COVID related delays, or other issues. 16 teams qualify for the playoffs and play in a four-round, single elimination bracket. Our analysis will include both playoff and regular season games, and will not differentiate between the two. \n","\n","Our analysis will contain many variables to help predict the outcome of games. Some hockey terms that are helpful to know include the following:\n","\n","- Shots - Shots in hockey are defined as shots on goal that the goalie saves.\n","\n","- Hits - A hit is when a player removes an opposing player from the puck, usually by running into them.\n","\n","- PIM - Stands for penalties in minutes. Players who receive a penalty usually go to the penalty box for a designated period of time.\n","\n","- Powerplay - When one team gets a penalty the other team will get to play with more players on the ice than them, as one or more of their players are in the penalty box. This is called a powerplay. \n","\n","- Faceoff - When the referee drops the puck to start a play, and two players compete to win possession of it. \n","\n","- Giveaway - When a player's own actions result in the loss of the puck\n","\n","- Takeaway - When a player's actions on the team without the puck result in them taking the puck from a player on the team with it. \n","\n","- Points - A hockey team earns 2 points for a win, 0 points for a loss, and 1 point for a loss in overtime or a shootout.\n","\n","- Point Percentage - Points/(Games Played * 2)"],"metadata":{"id":"fRuow2MgjzMa"}},{"cell_type":"markdown","source":["## Project Goal"],"metadata":{"id":"A1pK3tboXJ3s"}},{"cell_type":"markdown","source":["The goal of this project is to create a model that can successfully predict the winner of NHL games better statistically better than 50% accuracy. (Once we get the number of games we are predicting on we can do a 99% confidence interval for better than 50%). "],"metadata":{"id":"qQtN9phoj0MS"}},{"cell_type":"markdown","source":["# Notebook Configuration"],"metadata":{"id":"tX1i8YVHWimM"}},{"cell_type":"markdown","source":["Copied over the usual setup, though we may want to change some later (like the matplotlib stuff)"],"metadata":{"id":"ygMoK6kVZn_k"}},{"cell_type":"markdown","metadata":{"id":"H3yB94KtgMHu"},"source":["## Google drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jWmYBTOwgNs-","outputId":"8d52e3ee-f4be-47e6-dade-bbb152b39055","scrolled":true,"executionInfo":{"status":"ok","timestamp":1649716011079,"user_tz":240,"elapsed":17159,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import sys\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Get the absolute path of the current folder\n","abspath_curr = '/content/drive/My Drive/Colab Notebooks/teaching/gwu/machine_learning_I/homework/homework_5/'\n","\n","# Get the absolute path of the deep utilities folder\n","abspath_util_deep = '/content/drive/My Drive/Colab Notebooks/teaching/gwu/machine_learning_I/code/utilities/p3_deep_learning/'"]},{"cell_type":"markdown","metadata":{"id":"bYZhU1Wqgmqx"},"source":["## Warning"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"MUl4k83e4ANR","executionInfo":{"status":"ok","timestamp":1649790870405,"user_tz":240,"elapsed":5,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"outputs":[],"source":["import warnings\n","\n","# Ignore warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"6WMODpPfgn2U"},"source":["## Matplotlib"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"DBRVH9SB4ANb","executionInfo":{"status":"ok","timestamp":1649790875021,"user_tz":240,"elapsed":170,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline \n","\n","# Set matplotlib sizes\n","plt.rc('font', size=20)\n","plt.rc('axes', titlesize=20)\n","plt.rc('axes', labelsize=20)\n","plt.rc('xtick', labelsize=20)\n","plt.rc('ytick', labelsize=20)\n","plt.rc('legend', fontsize=20)\n","plt.rc('figure', titlesize=20)"]},{"cell_type":"markdown","metadata":{"id":"n-wNDk5nZhhO"},"source":["## TensorFlow\n","\n","- don't think we need this"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjG43tEnZkfE"},"outputs":[],"source":["# The magic below allows us to use tensorflow version 2.x\n","%tensorflow_version 2.x \n","import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"markdown","metadata":{"id":"40FN3UNfO2Z7"},"source":["## Random seed"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"uSADk0hJP71d","executionInfo":{"status":"ok","timestamp":1649790920925,"user_tz":240,"elapsed":143,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"outputs":[],"source":["# The random seed\n","random_seed = 42\n","\n","# Set random seed in tensorflow\n","#tf.random.set_seed(random_seed)\n","\n","# Set random seed in numpy\n","import numpy as np\n","np.random.seed(random_seed)"]},{"cell_type":"markdown","source":["# Data Preprocessing"],"metadata":{"id":"FdLHdRNRWveb"}},{"cell_type":"markdown","source":["## Importing"],"metadata":{"id":"bsAAp_-U4gLz"}},{"cell_type":"markdown","source":["We took the data initially hosted on Kaggle and moved it to Github to make it easier to import."],"metadata":{"id":"TANgdsE8YfUm"}},{"cell_type":"code","source":["import pandas as pd\n","\n","game = pd.read_csv('https://raw.githubusercontent.com/adamkritz/nhlgames/main/Data/game.csv')\n","team_stats = pd.read_csv('https://raw.githubusercontent.com/adamkritz/nhlgames/main/Data/game_teams_stats.csv')\n","team_info = pd.read_csv('https://raw.githubusercontent.com/adamkritz/nhlgames/main/Data/team_info.csv')"],"metadata":{"id":"VZlqqcD9e2YV","executionInfo":{"status":"ok","timestamp":1649790926220,"user_tz":240,"elapsed":2028,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Initial Feature Selection and Cleaning\n","\n","Games were included only if they were official games (Regular Season or Playoffs) and if they had an official outcome (settled in Regulation or Overtime)."],"metadata":{"id":"4msPvSMWvYmZ"}},{"cell_type":"code","source":["game = game[game['type'].isin(['R','P'])]\n","team_stats = team_stats[team_stats['settled_in'].isin(['REG', 'OT'])]\n","datgame = game[['game_id', 'season', 'date_time_GMT']]\n","datteam = team_stats[['game_id', 'team_id', 'HoA', 'won', 'settled_in', 'head_coach', 'shots', 'hits', 'pim', 'powerPlayOpportunities', 'faceOffWinPercentage', 'giveaways', 'takeaways', 'blocked']]\n","\n","dat = datteam.merge(datgame, on = 'game_id', how = 'inner')\n","dat = dat.drop_duplicates(subset = ['game_id', 'team_id'])\n","\n","dat = dat.sort_values(by = ['team_id', 'date_time_GMT'])\n","dat['date_time_GMT'] = pd.to_datetime(dat['date_time_GMT'])"],"metadata":{"id":"4B7kBpT0vi5s","executionInfo":{"status":"ok","timestamp":1649793235803,"user_tz":240,"elapsed":329,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":["## Feature Engineering\n","\n","**created features**:\n","- points: The points recieved by the team for this game. 2 for a win, 1 for a loss in overtime, and 0 for a regulation loss\n","- last season points percentage: The points percentage (revieved points / total possible points) of the previous season.\n","- running points percentag: The points percentage of the team up to that point in the season, before going into the game in question.\n","- last 10 points percentage: The points percentage of the last 10 games of the current season."],"metadata":{"id":"cg_KHUXUwItH"}},{"cell_type":"code","source":["dat['pts'] = dat.apply(lambda x: 2 if x.won else 1 if not x.won and x.settled_in == 'OT' else 0, axis = 1)"],"metadata":{"id":"FCppK_wJw3Co","executionInfo":{"status":"ok","timestamp":1649793240207,"user_tz":240,"elapsed":1511,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["seasonsorted = sorted(dat['season'].unique())\n","dat['season_num'] = dat['season'].apply(lambda x: seasonsorted.index(x))\n","grouped = dat.groupby(['team_id', 'season_num']).agg({'game_id':'count', 'pts':'sum'})\n","grouped = grouped.reset_index()\n","grouped['pts_perc_last_szn'] = grouped['pts']/(grouped['game_id']*2)\n","grouped['season_num'] = grouped['season_num']+1\n","\n","to_merge = grouped[['team_id', 'season_num', 'pts_perc_last_szn']]\n","dat = dat.merge(to_merge, on = ['season_num', 'team_id'], how = 'left')"],"metadata":{"id":"3yvgNqgIxWBG","executionInfo":{"status":"ok","timestamp":1649793240211,"user_tz":240,"elapsed":13,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["dat['L10_pts_perc'] = dat.groupby(['team_id', 'season_num'])['pts'].transform(lambda x: x.rolling(10,10).sum()/20)\n","dat['rolling_pts%'] = dat.groupby(['team_id', 'season_num']).apply(lambda x: x.pts.expanding().sum()/(x.pts.expanding().count()*2)).values"],"metadata":{"id":"W689im9XxaUK","executionInfo":{"status":"ok","timestamp":1649793241576,"user_tz":240,"elapsed":859,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":["## Transformations\n","\n","- The 'home' value was transformed from 'H' (home) and 'A' (away) to a binary value where 1 is home and 0 is away.\n","- The time stamp of the game was transformed into total seconds (unix time)\n","- The head coach column became the running points percentage of the coach until that point, spanning their entire career and all teams.\n","- 'Won' column from binary to 0 and 1"],"metadata":{"id":"uOKW-ecpx1Aq"}},{"cell_type":"code","source":["dat['home'] = dat['HoA'].apply(lambda x: 1 if x=='home' else 0)"],"metadata":{"id":"Av5HoZ4Xx3Ds","executionInfo":{"status":"ok","timestamp":1649793243089,"user_tz":240,"elapsed":182,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["dat['gametime_unix'] = dat['date_time_GMT'].apply(lambda x: x.timestamp())"],"metadata":{"id":"Dp3kUdCOyklU","executionInfo":{"status":"ok","timestamp":1649793244253,"user_tz":240,"elapsed":501,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["coach_group = dat.groupby(['head_coach']).apply(lambda x: x.won.expanding().sum()/(x.won.expanding().count()*2))\n","coach_flat = coach_group.reset_index().rename(columns = {'won':'coach_pts%'})\n","dat = dat.merge(coach_flat, left_index=True, right_on = 'level_1')\n"],"metadata":{"id":"0e7kxSTL9J8T","executionInfo":{"status":"ok","timestamp":1649793244869,"user_tz":240,"elapsed":168,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["dat['won'] = dat['won'].astype(int)"],"metadata":{"id":"AWRTSQHHTljT","executionInfo":{"status":"ok","timestamp":1649793250259,"user_tz":240,"elapsed":179,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":["## Remove Unecessary Columns"],"metadata":{"id":"BPrZFCo09TmT"}},{"cell_type":"code","source":["df = dat.drop(['HoA', 'head_coach_x', 'date_time_GMT', 'season', 'head_coach_y', 'level_1', 'settled_in'], axis = 1)"],"metadata":{"id":"rzgzLBui9WxN","executionInfo":{"status":"ok","timestamp":1649793253226,"user_tz":240,"elapsed":158,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":["## Splitting Data"],"metadata":{"id":"7wLzuwcHNalm"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","df_train, df_test = train_test_split(df, train_size = 0.8, random_state = random_seed)\n","\n","# Divide the training data into training (80%) and validation (20%)\n","df_train, df_val = train_test_split(df_train, train_size=0.8, random_state=random_seed)\n","\n","# Reset the index\n","df_train, df_val = df_train.reset_index(drop=True), df_val.reset_index(drop=True)\n","\n","target = 'won'"],"metadata":{"id":"CrTuqqv6NcEi","executionInfo":{"status":"ok","timestamp":1649793254056,"user_tz":240,"elapsed":125,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["# getting the shape of the data\n","\n","print('Train shape: ', df_train.shape)\n","print('Test shape: ', df_test.shape)\n","print('Val shape: ', df_val.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HGEV-D2IOToP","executionInfo":{"status":"ok","timestamp":1649793256153,"user_tz":240,"elapsed":191,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}},"outputId":"4fbcb5d8-2957-43c0-f0ba-ba6146dd0298"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape:  (30356, 19)\n","Test shape:  (9487, 19)\n","Val shape:  (7589, 19)\n"]}]},{"cell_type":"markdown","source":["## Checking Features"],"metadata":{"id":"rbI-swdoOYTv"}},{"cell_type":"markdown","source":["### Uncommon Features"],"metadata":{"id":"JCdLlA0XQWRQ"}},{"cell_type":"code","source":["## getting common features\n","common_features = np.intersect1d(np.intersect1d(df_train.columns, df_val.columns), df_test.columns)\n","print(common_features)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dV3lx0IxOc3-","executionInfo":{"status":"ok","timestamp":1649793257056,"user_tz":240,"elapsed":135,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}},"outputId":"6022e1c7-e8b4-492b-a742-84a31252c60c"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["['L10_pts_perc' 'blocked' 'coach_pts%' 'faceOffWinPercentage' 'game_id'\n"," 'gametime_unix' 'giveaways' 'hits' 'home' 'pim' 'powerPlayOpportunities'\n"," 'pts' 'pts_perc_last_szn' 'rolling_pts%' 'season_num' 'shots' 'takeaways'\n"," 'team_id' 'won']\n"]}]},{"cell_type":"code","source":["uncommon = np.setdiff1d(df_train.columns, common_features) + np.setdiff1d(df_val.columns, common_features) + np.setdiff1d(df_test.columns, common_features)\n","print(uncommon)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N5b8Z3WjPmct","executionInfo":{"status":"ok","timestamp":1649793259940,"user_tz":240,"elapsed":148,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}},"outputId":"90bed868-2aa5-4b95-beca-314c7ca26499"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n"]}]},{"cell_type":"markdown","source":["kewl, no uncommon features, probs didn't need this cause we saw they all had the same number of columns, ah well"],"metadata":{"id":"juVsKtBiQEdC"}},{"cell_type":"markdown","source":["### Identifiers"],"metadata":{"id":"4SKRtosjQS2o"}},{"cell_type":"code","source":["id_cols = ['game_id']\n","\n","df_train.drop(columns = id_cols, inplace = True)\n","df_test.drop(columns = id_cols, inplace = True)\n","df_val.drop(columns = id_cols, inplace = True)\n","\n","print('Train shape: ', df_train.shape)\n","print('Test shape: ', df_test.shape)\n","print('Val shape: ', df_val.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kA0MTqP9QmuC","executionInfo":{"status":"ok","timestamp":1649793261233,"user_tz":240,"elapsed":122,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}},"outputId":"906d0051-b9ec-42a1-a2f5-3cedef39017e"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape:  (30356, 18)\n","Test shape:  (9487, 18)\n","Val shape:  (7589, 18)\n"]}]},{"cell_type":"markdown","source":["## Missing Data"],"metadata":{"id":"qfR0UWzfRg6z"}},{"cell_type":"code","source":["# Combine df_train, df_val and df_test\n","df = pd.concat([df_train, df_val, df_test], sort=False)"],"metadata":{"id":"7B3IibyBQwlr","executionInfo":{"status":"ok","timestamp":1649793263467,"user_tz":240,"elapsed":162,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["def nan_checker(df):\n","    # JAMES'S CODE !! Just so i can use the func, figure out how to use this later\n","    \"\"\"\n","    The NaN checker\n","\n","    Parameters\n","    ----------\n","    df : the dataframe\n","    \n","    Returns\n","    ----------\n","    The dataframe of variables with NaN, their proportion of NaN and data type\n","    \"\"\"\n","    \n","    # Get the dataframe of variables with NaN, their proportion of NaN and data type\n","    df_nan = pd.DataFrame([[var, df[var].isna().sum() / df.shape[0], df[var].dtype]\n","                           for var in df.columns if df[var].isna().sum() > 0],\n","                          columns=['var', 'proportion', 'dtype'])\n","    \n","    # Sort df_nan in accending order of the proportion of NaN\n","    df_nan = df_nan.sort_values(by='proportion', ascending=False).reset_index(drop=True)\n","    \n","    return df_nan"],"metadata":{"id":"W-BjCvxUSThY","executionInfo":{"status":"ok","timestamp":1649793264875,"user_tz":240,"elapsed":138,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["df_miss = nan_checker(df)\n","\n","# Print df_nan\n","df_miss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"id":"vjYDMLqpSZsc","executionInfo":{"status":"ok","timestamp":1649793267317,"user_tz":240,"elapsed":130,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}},"outputId":"e65a3fff-f53e-46e3-8f1e-2583468a903f"},"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                    var  proportion    dtype\n","0  faceOffWinPercentage    0.466773  float64\n","1          L10_pts_perc    0.108724  float64\n","2                  hits    0.103727  float64\n","3             giveaways    0.103727  float64\n","4             takeaways    0.103727  float64\n","5               blocked    0.103727  float64\n","6     pts_perc_last_szn    0.057472  float64"],"text/html":["\n","  <div id=\"df-8406e7b1-6455-4cff-886c-6f86624abb09\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>var</th>\n","      <th>proportion</th>\n","      <th>dtype</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>faceOffWinPercentage</td>\n","      <td>0.466773</td>\n","      <td>float64</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>L10_pts_perc</td>\n","      <td>0.108724</td>\n","      <td>float64</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>hits</td>\n","      <td>0.103727</td>\n","      <td>float64</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>giveaways</td>\n","      <td>0.103727</td>\n","      <td>float64</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>takeaways</td>\n","      <td>0.103727</td>\n","      <td>float64</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>blocked</td>\n","      <td>0.103727</td>\n","      <td>float64</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>pts_perc_last_szn</td>\n","      <td>0.057472</td>\n","      <td>float64</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8406e7b1-6455-4cff-886c-6f86624abb09')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8406e7b1-6455-4cff-886c-6f86624abb09 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8406e7b1-6455-4cff-886c-6f86624abb09');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":74}]},{"cell_type":"markdown","source":["All the missing data are floats so we can consider them missing and impute them"],"metadata":{"id":"B2pZWkqRSywJ"}},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer\n","\n","# The SimpleImputer\n","si = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n","\n","# Impute the variables with missing values in df_train, df_val and df_test \n","df_train[df_miss['var']] = si.fit_transform(df_train[df_miss['var']])\n","df_val[df_miss['var']] = si.transform(df_val[df_miss['var']])\n","df_test[df_miss['var']] = si.transform(df_test[df_miss['var']])"],"metadata":{"id":"ogxf0W4cS34A","executionInfo":{"status":"ok","timestamp":1649793269691,"user_tz":240,"elapsed":121,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":["## Encoding"],"metadata":{"id":"8UFYZbx7TSSu"}},{"cell_type":"code","source":["# Combine df_train, df_val and df_test\n","df = pd.concat([df_train, df_val, df_test], sort=False)\n","\n","# Print the unique data type of variables in df\n","pd.DataFrame(df.dtypes.unique(), columns=['dtype'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"u6XCnAm0TTfK","executionInfo":{"status":"ok","timestamp":1649793273134,"user_tz":240,"elapsed":148,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}},"outputId":"d88641a3-b9f0-428c-a22e-13fae511d681"},"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     dtype\n","0    int64\n","1  float64"],"text/html":["\n","  <div id=\"df-26217ae4-d4f9-431d-bdf9-b0ff2833dba1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>dtype</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>int64</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>float64</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26217ae4-d4f9-431d-bdf9-b0ff2833dba1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-26217ae4-d4f9-431d-bdf9-b0ff2833dba1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-26217ae4-d4f9-431d-bdf9-b0ff2833dba1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":76}]},{"cell_type":"markdown","source":["No cat variables, woo, no need to encode"],"metadata":{"id":"R4QqRzxIV_Qb"}},{"cell_type":"markdown","source":["## Split Features and Target"],"metadata":{"id":"cxU8hxNEWICr"}},{"cell_type":"code","source":["# Get the feature matrix\n","X_train = df_train[np.setdiff1d(df_train.columns, [target])].values\n","X_val = df_val[np.setdiff1d(df_val.columns, [target])].values\n","X_test = df_test[np.setdiff1d(df_test.columns, [target])].values\n","\n","# Get the target vector\n","y_train = df_train[target].values\n","y_val = df_val[target].values\n","y_test = df_test[target].values"],"metadata":{"id":"1H1Lp3WOWKAy","executionInfo":{"status":"ok","timestamp":1649793345118,"user_tz":240,"elapsed":135,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":["## Scale"],"metadata":{"id":"yCahqFH9WL02"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","# The StandardScaler\n","ss = StandardScaler()"],"metadata":{"id":"uZIz29UyWNJ-","executionInfo":{"status":"ok","timestamp":1649793367818,"user_tz":240,"elapsed":120,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["# Standardize the training data\n","X_train = ss.fit_transform(X_train)\n","\n","# Standardize the validation data\n","X_val = ss.transform(X_val)\n","\n","# Standardize the test data\n","X_test = ss.transform(X_test)"],"metadata":{"id":"F4sdnBDyWQY3","executionInfo":{"status":"ok","timestamp":1649793377955,"user_tz":240,"elapsed":171,"user":{"displayName":"Sahara Ensley","userId":"14095096302141685125"}}},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":["# Training\n","\n","This code was taken from: https://github.com/yuxiaohuang/teaching/blob/master/gwu/machine_learning_I/spring_2022/code/p2_shallow_learning/p2_c2_supervised_learning/p2_c2_s5_tree_based_models/case_study/case_study.ipynb \n","\n","These three models are a culmination of all the models we learned for classification. It has logistic regression, shallow neural networks, and random forest classifier (and als the histgradientboosting one). This was used to classify titanic survival as Yes or No, so it will probably be good for us."],"metadata":{"id":"1RYCaKJSWxGj"}},{"cell_type":"markdown","metadata":{"id":"jYCH8GB-e31z"},"source":["## Creating the dictionary of the models\n","- In the dictionary:\n","    - the key is the acronym of the model\n","    - the value is the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T5h3IQo9e31z"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.experimental import enable_hist_gradient_boosting\n","from sklearn.ensemble import HistGradientBoostingClassifier\n","\n","models = {'lr': LogisticRegression(class_weight='balanced', random_state=random_seed),\n","          'mlpc': MLPClassifier(early_stopping=True, random_state=random_seed),\n","          'rfc': RandomForestClassifier(class_weight='balanced', random_state=random_seed),\n","          'hgbc': HistGradientBoostingClassifier(random_state=random_seed)}"]},{"cell_type":"markdown","metadata":{"id":"Ju9FdEVce310"},"source":["## Creating the dictionary of the pipelines\n","In the dictionary:\n","- the key is the acronym of the model\n","- the value is the pipeline, which, for now, only includes the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zc-L-914e310"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","\n","pipes = {}\n","\n","for acronym, model in models.items():\n","    pipes[acronym] = Pipeline([('model', model)])"]},{"cell_type":"markdown","metadata":{"id":"BgkFbGGKe311"},"source":["## Getting the predefined split cross-validator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6FAACb8ke311","scrolled":true},"outputs":[],"source":["# Get the:\n","# feature matrix and target velctor in the combined training and validation data\n","# target vector in the combined training and validation data\n","# PredefinedSplit\n","# See the implementation in pmlm_utilities.ipynb\n","X_train_val, y_train_val, ps = get_train_val_ps(X_train, y_train, X_val, y_val)"]},{"cell_type":"markdown","metadata":{"id":"I3rZoMqZe312"},"source":["## GridSearchCV"]},{"cell_type":"markdown","metadata":{"id":"E08xqUQve313"},"source":["### Creating the dictionary of the parameter grids\n","- In the dictionary:\n","    - the key is the acronym of the model\n","    - the value is the parameter grid of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p3OWujL3e313"},"outputs":[],"source":["param_grids = {}"]},{"cell_type":"markdown","metadata":{"id":"9EyKhepjmV6P"},"source":["#### The parameter grid for LogisticRegression\n","- The hyperparameters we want to fine-tune are:\n","    - tol\n","    - C\n","- See details of the meaning of the hyperparametes in [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i2Mv1rffnJpN"},"outputs":[],"source":["# The parameter grid of tol\n","tol_grid = [10 ** -5, 10 ** -4, 10 ** -3]\n","\n","# The parameter grid of C\n","C_grid = [0.1, 1, 10]\n","\n","# Update param_grids\n","param_grids['lr'] = [{'model__tol': tol_grid,\n","                      'model__C': C_grid}]"]},{"cell_type":"markdown","metadata":{"id":"bco06XPK4APO"},"source":["#### The parameter grid for MLPClassifier\n","- The hyperparameters we want to fine-tune are:\n","    - alpha\n","    - learning_rate_init\n","\n","- See details of the meaning of the hyperparametes in [sklearn.neural_network.MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bEuags7e4APO"},"outputs":[],"source":["# The grids for alpha\n","alpha_grids = [10 ** i for i in range(-5, -2)]\n","\n","# The grids for learning_rate_init\n","learning_rate_init_grids = [10 ** i for i in range(-4, -1)]\n","\n","# Update param_grids\n","param_grids['mlpc'] = [{'model__alpha': alpha_grids,\n","                        'model__learning_rate_init': learning_rate_init_grids}]"]},{"cell_type":"markdown","metadata":{"id":"T6Synl8u9cC8"},"source":["#### The parameter grid for random forest\n","- The hyperparameters we want to fine-tune are:\n","    - min_samples_split\n","    - min_samples_leaf\n","\n","- See details of the meaning of the hyperparametes in [sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2pIhWeST9cC8"},"outputs":[],"source":["# The grids for min_samples_split\n","min_samples_split_grids = [2, 20, 100]\n","\n","# The grids for min_samples_leaf\n","min_samples_leaf_grids = [1, 20, 100]\n","\n","# Update param_grids\n","param_grids['rfc'] = [{'model__min_samples_split': min_samples_split_grids,\n","                       'model__min_samples_leaf': min_samples_leaf_grids}]"]},{"cell_type":"markdown","metadata":{"id":"dmuPjEKN9cC-"},"source":["#### The parameter grid for histogram-based gradient boosting\n","- The hyperparameters we want to fine-tune are:\n","- learning_rate\n","- min_samples_leaf\n","\n","- See details of the meaning of the hyperparametes in [sklearn.ensemble.HistGradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FAj6ezza9cC-"},"outputs":[],"source":["# The grids for learning_rate\n","learning_rate_grids = [10 ** i for i in range(-3, 2)]\n","\n","# The grids for min_samples_leaf\n","min_samples_leaf_grids = [1, 20, 100]\n","\n","# Update param_grids\n","param_grids['hgbc'] = [{'model__learning_rate': learning_rate_grids,\n","                        'model__min_samples_leaf': min_samples_leaf_grids}]"]},{"cell_type":"markdown","metadata":{"id":"LRabeGs4WaHX"},"source":["### Creating the directory for the cv results produced by GridSearchCV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uO7Yd15JWaHY"},"outputs":[],"source":["# Make directory\n","directory = os.path.dirname(abspath_curr + '/result/titanic/cv_results/GridSearchCV/')\n","if not os.path.exists(directory):\n","    os.makedirs(directory)"]},{"cell_type":"markdown","metadata":{"id":"sVIuo08Ze31-"},"source":["### Tuning the hyperparameters\n","The code below shows how to fine-tune the hyperparameters of SGDRegressor and LinearRegression_MBGD using sklearn GridSearchCV."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"DwA4WZlFe31-","outputId":"e8a5f8fc-0f40-499d-85f3-be192bb5534d","scrolled":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>best_score</th>\n","      <th>best_param</th>\n","      <th>best_estimator</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.827220</td>\n","      <td>{'model__learning_rate': 0.1, 'model__min_samp...</td>\n","      <td>(HistGradientBoostingClassifier(l2_regularizat...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.802505</td>\n","      <td>{'model__min_samples_leaf': 1, 'model__min_sam...</td>\n","      <td>((DecisionTreeClassifier(ccp_alpha=0.0, class_...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.773968</td>\n","      <td>{'model__alpha': 1e-05, 'model__learning_rate_...</td>\n","      <td>(MLPClassifier(activation='relu', alpha=1e-05,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.765205</td>\n","      <td>{'model__C': 1, 'model__tol': 1e-05}</td>\n","      <td>(LogisticRegression(C=1, class_weight='balance...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   best_score  ...                                     best_estimator\n","0    0.827220  ...  (HistGradientBoostingClassifier(l2_regularizat...\n","1    0.802505  ...  ((DecisionTreeClassifier(ccp_alpha=0.0, class_...\n","2    0.773968  ...  (MLPClassifier(activation='relu', alpha=1e-05,...\n","3    0.765205  ...  (LogisticRegression(C=1, class_weight='balance...\n","\n","[4 rows x 3 columns]"]},"execution_count":62,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["from sklearn.model_selection import GridSearchCV\n","\n","# The list of [best_score_, best_params_, best_estimator_] obtained by GridSearchCV\n","best_score_params_estimator_gs = []\n","\n","# For each model\n","for acronym in pipes.keys():\n","    # GridSearchCV\n","    gs = GridSearchCV(estimator=pipes[acronym],\n","                      param_grid=param_grids[acronym],\n","                      scoring='f1_macro',\n","                      n_jobs=2,\n","                      cv=ps,\n","                      return_train_score=True)\n","        \n","    # Fit the pipeline\n","    gs = gs.fit(X_train_val, y_train_val)\n","    \n","    # Update best_score_params_estimator_gs\n","    best_score_params_estimator_gs.append([gs.best_score_, gs.best_params_, gs.best_estimator_])\n","    \n","    # Sort cv_results in ascending order of 'rank_test_score' and 'std_test_score'\n","    cv_results = pd.DataFrame.from_dict(gs.cv_results_).sort_values(by=['rank_test_score', 'std_test_score'])\n","    \n","    # Get the important columns in cv_results\n","    important_columns = ['rank_test_score',\n","                         'mean_test_score', \n","                         'std_test_score', \n","                         'mean_train_score', \n","                         'std_train_score',\n","                         'mean_fit_time', \n","                         'std_fit_time',                        \n","                         'mean_score_time', \n","                         'std_score_time']\n","    \n","    # Move the important columns ahead\n","    cv_results = cv_results[important_columns + sorted(list(set(cv_results.columns) - set(important_columns)))]\n","\n","    # Write cv_results file\n","    cv_results.to_csv(path_or_buf=abspath_curr + '/result/titanic/cv_results/GridSearchCV/' + acronym + '.csv', index=False)\n","\n","# Sort best_score_params_estimator_gs in descending order of the best_score_\n","best_score_params_estimator_gs = sorted(best_score_params_estimator_gs, key=lambda x : x[0], reverse=True)\n","\n","# Print best_score_params_estimator_gs\n","pd.DataFrame(best_score_params_estimator_gs, columns=['best_score', 'best_param', 'best_estimator'])"]},{"cell_type":"markdown","source":["# Testing\n","\n","Here we will select best_estimator_gs as the best model. Later we will use this best model to generate the submission file for this kaggle competition."],"metadata":{"id":"jonCCJgpW1F7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjbLGmXPe32J"},"outputs":[],"source":["# Get the best_score, best_params and best_estimator obtained by GridSearchCV\n","best_score_gs, best_params_gs, best_estimator_gs = best_score_params_estimator_gs[0]"]},{"cell_type":"markdown","source":["# Model Evaluation"],"metadata":{"id":"jTu3oSwQW2oU"}},{"cell_type":"markdown","source":["# Conclusion"],"metadata":{"id":"0RPVlucLaz8k"}}]}